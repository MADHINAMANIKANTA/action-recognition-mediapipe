# -*- coding: utf-8 -*-
"""randomforest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WEnef_VUgkA3OFj7RCqRsl8PJb8CWMwn
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile # import the zipfile module

zip_file_path = "/content/drive/My Drive/N-UCLA-RGB.zip"
extract_to = "/content/N-UCLA-RGB"
try:
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print("File successfully extracted to:", extract_to)
except zipfile.BadZipFile:
    print("BadZipFile: The uploaded file is not a valid zip file or is corrupted.")

import os

extract_to = "/content/N-UCLA-RGB"
for root, dirs, files in os.walk(extract_to):
    for file in files:
        print(os.path.join(root, file))

import os

data_path = "/content/N-UCLA-RGB/multiview_action_videos"

# List all directories and files
for root, dirs, files in os.walk(data_path):
    print(f"Directory: {root}, Subdirectories: {dirs}, Files: {files}")

import os

# Actions to include
selected_actions = {
    "a01": "sitting",
    "a02": "standing",
    "a03": "walking",
    "a09": "punching",
    "a12": "waving"
}

# Directory containing videos
data_path = "/content/N-UCLA-RGB/multiview_action_videos"

# Filter videos based on selected actions
filtered_videos = []
for root, _, files in os.walk(data_path):
    label_code = os.path.basename(root)
    if label_code in selected_actions:
        for file in files:
            video_path = os.path.join(root, file)
            label = selected_actions[label_code]
            filtered_videos.append((video_path, label))

# Print filtered videos
for video, label in filtered_videos:
    print(f"Video: {video}, Label: {label}")

# Check the number of videos for each action
from collections import Counter
label_counts = Counter(label for _, label in filtered_videos)
print("\nNumber of videos per action:")
for label, count in label_counts.items():
    print(f"{label}: {count}")

!pip install mediapipe

import pandas as pd

# Convert the filtered videos list into a DataFrame
videos_df = pd.DataFrame(filtered_videos, columns=["Video_Path", "Label"])

# Now, you can filter videos based on the selected actions
filtered_videos_df = videos_df[videos_df["Label"].isin(["sitting", "standing", "walking", "punching", "waving"])]

# Iterate over the filtered videos and process them
for _, row in filtered_videos_df.iterrows():
    video_path = row["Video_Path"]
    label = row["Label"]
    print(f"Processing video: {video_path}, Label: {label}")

# Assuming videos_df contains all video paths and labels
filtered_videos_df = videos_df[videos_df["Label"].isin(["sitting", "standing", "walking", "punching", "waving"])]

for _, row in filtered_videos_df.iterrows():
    video_path = row["Video_Path"]  # Ensure column name is correct
    label = row["Label"]

# Correct the column name in your code
for _, row in filtered_videos_df.iterrows():
    video_path = row["Video_Path"]  # Use "Video_Path" instead of "video_path"
    label = row["Label"]

    # Perform your processing here
    print(f"Processing video: {video_path}, Label: {label}")

import cv2
import mediapipe as mp

mp_pose = mp.solutions.pose

for _, row in filtered_videos_df.iterrows():
    video_path = row["Video_Path"]  # Use "Video_Path"
    label = row["Label"]
    print(f"Processing video: {video_path}, Label: {label}")

    # Step 1: Read the video
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame_count >= 100:  # Limit to 100 frames for testing
                break

            # Step 2: Convert the frame to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Step 3: Process frame for pose landmarks
            results = pose.process(frame_rgb)

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                print(f"Frame {frame_count}:")
                for idx, lm in enumerate(landmarks):
                    print(f"Landmark {idx}: (x={lm.x}, y={lm.y}, z={lm.z})")

            frame_count += 1

    cap.release()

extracted_landmarks = []  # Create a list to store landmarks and labels

for _, row in filtered_videos_df.iterrows():
    video_path = row["Video_Path"]
    label = row["Label"]
    print(f"Processing video: {video_path}, Label: {label}")

    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame_count >= 100:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(frame_rgb)

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                extracted_landmarks.append((landmarks, label))  # Store the landmarks and label

            frame_count += 1

    cap.release()

# Flatten the landmarks and store as rows
landmark_data = []

for landmarks, label in extracted_landmarks:
    landmark_row = []
    for lm in landmarks:
        landmark_row.extend([lm.x, lm.y, lm.z])  # Flatten landmarks
    landmark_row.append(label)  # Append the label
    landmark_data.append(landmark_row)

# Create a DataFrame
columns = [f"Landmark_{i}_{axis}" for i in range(33) for axis in ["x", "y", "z"]] + ["Label"]
landmark_df = pd.DataFrame(landmark_data, columns=columns)
print(landmark_df.head())

from sklearn.preprocessing import MinMaxScaler

# Normalize only landmark columns (exclude label)
scaler = MinMaxScaler()
landmark_df.iloc[:, :-1] = scaler.fit_transform(landmark_df.iloc[:, :-1])

print(landmark_df.head())  # Check normalized data

from sklearn.model_selection import train_test_split

X = landmark_df.drop("Label", axis=1)  # Features (landmarks)
y = landmark_df["Label"]  # Labels

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training samples: {len(X_train)}, Testing samples: {len(X_test)}")

from sklearn.svm import SVC

# Train SVM model
svm_model = SVC(kernel="rbf", C=1, gamma="scale")  # You can tune these hyperparameters
svm_model.fit(X_train, y_train)

print("Model training complete.")

from sklearn.metrics import accuracy_score, classification_report

y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Classification report for more insights
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

#start
landmark_df_diff = landmark_df.iloc[:, :-1].diff().fillna(0)  # Compute velocity features
landmark_df = pd.concat([landmark_df, landmark_df_diff.add_suffix('_diff')], axis=1)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print(classification_report(y_test, y_pred_rf))

from sklearn.ensemble import RandomForestClassifier

best_rf = RandomForestClassifier(n_estimators=100, random_state=42)
best_rf.fit(X_train, y_train)

# Now, save the trained model
import joblib
joblib.dump(best_rf, "random_forest_model.pkl")

best_rf = joblib.load("random_forest_model.pkl")
y_pred = best_rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")