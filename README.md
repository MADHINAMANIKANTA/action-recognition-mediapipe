# action-recognition-mediapipe
# Real-Time Action Recognition in Multi-Person Pose Estimation Using MediaPipe

This project focuses on recognizing human actions in real time using multi-person pose estimation. We use **MediaPipe** to extract 2D skeletal landmarks, **YOLOv8** for multiple person detection, and train a **Random Forest** model to classify actions.

---

## ğŸ¯ Recognized Actions
- ğŸ‘Š Punching  
- ğŸ™‡ Sitting  
- ğŸ™† Standing  
- ğŸš¶ Walking  
- ğŸ‘‹ Waving  

---

## ğŸ§  Model and Dataset

- **Model Used**: Random Forest Classifier  
- **Pose Estimation**: MediaPipe (BlazePose)  
- **Person Detection**: YOLOv8  
- **Dataset**: [N-UCLA RGB Dataset (Kaggle)](https://www.kaggle.com/datasets/akshayjain22/n-ucla-rgb)  
- **Accuracy Achieved**: â­ 86% on selected actions  

---

## ğŸ“ Folder Structure
RealTime-Action-Recognition/
â”‚
â”œâ”€â”€ action_model.py # Main real-time recognition script
â”œâ”€â”€ trained_model.pkl # Trained Random Forest model (if small enough)
â”œâ”€â”€ sample_videos/ # Few test videos (optional)
â”œâ”€â”€ requirements.txt # Python libraries used
â””â”€â”€ README.md # This file

## â–¶ï¸ How to Run

```bash
pip install -r requirements.txt
python action_model.py
Make sure:

A webcam is connected.

The trained model is in the expected path.

YOLOv8 weights are available or downloaded via Ultralytics.
ğŸ‘¨â€ğŸ« Guided by: Dr. G.R.S. Murthy

ğŸ‘¨â€ğŸ’» Team Members

M. Manikanta
G. Mamatha Siri
P. Vasu
P. Varsha

ğŸ“¦ Requirements (partial)
opencv-python
mediapipe
ultralytics
scikit-learn
joblib
numpy
âš ï¸ Notes
Due to size, the full N-UCLA dataset and trained model are not included. Please refer to:

Dataset on Kaggle





